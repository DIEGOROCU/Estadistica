\section{Muestreo}

\subsection{Modelos estadísticos}

\label{Modelo estidístico}
\begin{definición}[Modelo Estadístico]
Sea $(\omega, \mathcal{A}, \mathbb{P})$ un espacio de probabilidad asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \omega \to \mathbb{R}$ y su espacio medible asociado $(\chi, \mathcal{B})$.
\\Un \textbf{modelo estadístico} es una terna $(\chi, \mathcal{B}, F)$, donde:
\vspace{-\topsep}
\begin{itemize}
	\item $\chi$ es el espacio donde la variable aleatoria $X$ toma valores.
	\item $\mathcal{B}$ es la $\sigma$-álgebra asociada a $\chi$. Generalmente se toma que $\mathcal{B} = \mathcal{B}(\mathbb{R})$.
	\item $F$ es el conjunto de todas las posibles funciones de distribución que podemos considerar sobre $\mathcal{B}$.
\end{itemize}
\end{definición}

\begin{definición}[Modelo Estadístico Paramétrico]
Un \textbf{modelo estadístico paramétrico} es, al igual que el anteior, una terna $(\chi, \mathcal{B}, F)$, pero en este caso $F$ depende de un parámetro $\theta$ desconocido. \\
Se define $F = \{F_{\theta} : \theta \in \Theta\}$, donde $\Theta$ es el espacio paramétrico, es decir, el conjunto de todos los posibles valores que puede tomar $\theta$ para que $F_\theta$ sea una función de distribución.
De forma general se tiene que $\theta \in \Theta \subseteq \mathbb{R}^{k}$. \\
Además se tienen dos enfoques según cómo se tome el comportamiento de $\theta$:
\vspace{-\topsep}
\begin{itemize}
	\item \textbf{Enfoque frecuentista:} $\theta$ es un valor fijo pero desconocido.
	\item \textbf{Enfoque bayesiano:} $\theta$ es una variable aleatoria.
\end{itemize}
\end{definición}

\ejemplo{
	\begin{enumerate}
		\item Sea $X$ una variable aleatoria con distribución $N(\theta, 1)$, donde $\theta$ es un parámetro desconocido. Entonces, el modelo estadístico asociado a este experimento es $(\mathbb{R}, \mathcal{B}, F_{\theta}, \theta)$, donde $F_{\theta}(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}(x-\theta)^2}$. En este caso $\Theta = \mathbb{R}$.
		\item Sea $X$-variable aleatoria con distribución $Bernouilli$, de parámetro $\theta$. Entonces en este caso, el modelo estadístico asociado es $(\{0,1\}, \mathcal{B}, F_{\theta}, \theta)$, donde $F_{\theta}(x) = \theta^x (1-\theta)^{1-x}$. En este caso $\Theta = [0,1]$.
	\end{enumerate}
}

\begin{definición}[Muestra aleatoria simple]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ un conjunto de variables aleatorias independientes e idénticamente distribuidas (i.i.d.) entonces a dicho conjunto se le conoce como \textbf{muestra aleatoria simple} de tamaño n.
\\ Por tanto tenemos que el modelo estadístico asociado es una terna $(\chi, \mathcal{B}, F)$, donde:
\vspace{-\topsep}
\begin{itemize}
	\item $\chi = \mathbb{R}^{n}$.
	\item $\mathcal{B} = \mathcal{B}(\mathbb{R}^{n})$.
	\item $F = \{F_{\theta}^n : \theta \in \Theta\}$, donde $\Theta$ es el espacio paramétrico, donde:
	      \[F_{\theta}^n(x_1, \ldots, x_n) = \prod_{i=1}^{n} F_{\theta}(x_i)\]
\end{itemize}
\end{definición}

\begin{definición}[Función de Distibución Empírica]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s. $(n) \sim X$ y denotemos por $\left(X_{(1)}, \ldots, X_{(n)}\right)$ a la muestra ordenada de menor a mayor. $\forall x \in \mathbb{R}$ fijo definimos la función distribución empírica como la variable aleatoria $$F_{n}(x)=\frac{1}{n} \sum_{i=1}^{n} I_{(-\infty, x]}\left(X_{i}\right)$$

Observemos que

$$
	F_{n}(x)= \begin{cases}0 & \text { si } x<X_{(1)} \\ \frac{k}{n} & \text { si } X_{(k)}<x<X_{(k+1)}, k=1, \ldots, n-1 \\ 1 & \text { si } x \geq X_{(n)}\end{cases}
$$

Dada una realización particular de la muestra $\left(x_{1}, \ldots,
	x_{n}\right), F_{n}(x)$ es una función de distribución asociada a una variable
aleatoria discreta que toma valores $\left(x_{(1)}, \ldots, x_{(n)}\right)$ con
función de masa $\left(\frac{1}{n}, \ldots, \frac{1}{n}\right)$
\end{definición}

\ejemplo{
	Supongamos que tenemos una muestra aleatoria ordenada de tamaño $n = 5: \\
		x_{(1)} = 2, x_{(2)} = 3, x_{(3)} = 5, x_{(4)} = 7 x_{(5)} = 9$. \\
	Entones, por como se define la FDE $F_n(x) = \frac{1}{5} \sum_{i=1}^{5} I_{(-\infty, x]}(X_i)$, tenemos que:
	\[
		F_n(x) = \begin{cases}
			0           & \text{si } x \geq 2     \\
			\frac{1}{5} & \text{si } 2 \geq x < 3 \\
			\frac{2}{5} & \text{si } 3 \geq x < 5 \\
			\frac{3}{5} & \text{si } 5 \geq x < 7 \\
			\frac{4}{5} & \text{si } 7 \geq x < 9 \\
			1           & \text{si } x \geq 9
		\end{cases}
	\]
	De manera que cada vez que $x$ alcanza un valor de la muestra, la función de distribución empírica aumenta en $\frac{1}{n} =0,2$.
}

\begin{proposición}[Propiedades de la Función de Distribución Empírica]
Sea una muestra aleatoria \( X_1, X_2, \dots, X_n \) de una variable aleatoria \( X \) con función de distribución \( F(x) \). Definimos la función de distribución empírica como:

$$
	F_n(x) = \frac{1}{n} \sum_{i=1}^{n} \chi_{(-\infty,x]}(X_i)
$$

donde \( \chi_{(-\infty,x]}(X_i) \) es la función indicadora que toma el valor \( 1 \) si \( X_i \leq x \) y \( 0 \) en caso contrario.

\begin{enumerate}
	\item \textbf{Interpretación probabilística:}
	      La función indicadora \( \chi_{(-\infty,x]}(X_i) \) sigue una distribución Bernoulli con parámetro \( F(x) \), es decir:

	      $$
		      \chi_{(-\infty,x]}(X_i) \sim \text{Bernoulli}(F(x))
	      $$

	      Por tanto también podemos afirmar que: $$\chi_{(-\infty,x]}(X_i) \sim \text{Bin}(1, F(x))$$

	      Además, la suma de estas variables sigue una distribución binomial:

	      $$ \sum_{i=1}^{n} \chi_{(-\infty,x]}(X_i) \sim \text{Bin}(n, F(x)) $$
	\item \textbf{Esperanza y varianza:}
	      Para un valor fijo de \( x \), se cumple que:

	      $$ E[F_n(x)] = F(x) $$

	      lo que indica que \( F_n(x) \) es un estimador insesgado de \( F(x) \).
	      La varianza está dada por:

	      $$ V[F_n(x)] = \frac{F(x)(1 - F(x))}{n} \underset{n \to \infty}{\longrightarrow} 0 $$

	\item \textbf{Convergencia:}
	      \begin{enumerate}
		      \item \textbf{Convergencia casi segura:}

		            $$ F_n(x) \underset{n \to \infty}{\xrightarrow{c.s.}} F(x) $$

		      \item \textbf{Convergencia en distribución:}
		            Se cumple la normalidad asintótica:

		            $$ \frac{F_n(x) - F(x)}{\sqrt{\frac{F(x)(1-F(x))}{n}}} \underset{n \to \infty}{\xrightarrow{d}} N(0,1) $$

	      \end{enumerate}

	\item \textbf{Intervalo de confianza para \( F(x) \):}
	      Dada una realización particular de la muestra \( (x_1, \dots, x_n) \), se puede construir un intervalo de confianza asintótico para \( F(x) \) de nivel \( 1 - \alpha \):

	      $$ IC_{1-\alpha}(F(x)) = \left( F_n(x) - \frac{z_{\alpha/2}}{2\sqrt{n}}, F_n(x) + \frac{z_{\alpha/2}}{2\sqrt{n}} \right) $$

	      donde \( z_{\alpha/2} \) es el cuantil de la distribución normal estándar.

\end{enumerate}
\end{proposición}

\begin{teorema}[de Glivenko-Cantelli]
	Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n) \sim X$ con función de distribucióm empírica $F_{n}(x)$ y sea $F(x)$ la función de distribución de $X$, es decir, de la población total. Entonces se cumple que:
	\[\lim _{n \rightarrow \infty} P\left(w: \operatorname{Sup}_{x}\left|F_{n}(x)-F(x)\right|<\epsilon\right)=1, \ \forall \epsilon>0\]
\end{teorema}

\begin{corolario}
	El Teorema de Glivenko-Cantelli permite realizar una técnica estadística denominda \textbf{método de sustitución (Plug-In)} la cual se basa en la sustitución de parámetros desconocidos por sus estimaciones sobre una muestra. Por ejemplo:
	\begin{enumerate}
		\item Se puede estimar la media poblacional $\mu$ por la media muestral $\bar{X} = \int x\partial{F_n(x)} = \frac{1}{n}\sum_{i = 1}^{n}x_i $.
		\item Se puede estimar la varianza poblacional $\sigma^2$ por la varianza muestral $S^2 = \int (x - \bar{x})dF_n(x) = \frac{1}{n}\sum_{i = 1}^{n}(x_i - \bar{x})^2 = \sigma_n^2$.
	\end{enumerate}
\end{corolario}

\subsection{Estadísticos muestrales}

\begin{definición}[Estadístico muestral]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n) \sim X$ y sea $T: \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}$ medible (integrable), bien definida y no dependiente de parámetros desconocidos, se le llama \textbf{estadístico muestral}
\end{definición}

\ejemplo{
	Desacamos los siguientes estadísticos muestrales:
	\begin{enumerate}
		\item Media muestral $T\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{n} \sum_{i=1}^{n}
			      X_{i}=\bar{X}$
		\item  Cuasivarianza muestral $T\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{n-1}
			      \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{2}=S_{n}^{2}$
		\item $T\left(X_{1}, \ldots, X_{n}\right)=\left(\bar{X}, S_{n}^{2}\right)$
	\end{enumerate}
}
\subsection{Momentos}

Sea $\left(X_{1}, \cdots X_{n}\right)$ m.a.s.(n) de $X, \ \mu=E[X]$ y
$\sigma=\sqrt{V(X)}$.
\begin{definición}[Momento Muestral]
Se define el momento muestral de orden $k$ respecto al origen como
\[a_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k}\]
y el momento muestral de orden $k$ respecto a la media como
\[b_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k}\]
\end{definición}

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{itemize}
	\item El momento muestral de orden 1 respecto al origen es la media muestral $(a_1 =
		      \bar{X})$.
	\item El momento muestral de orden 2 respecto a la media es la varianza muestral
	      $(b_2 = \sigma_n^2)$.
\end{itemize}
\end{observación}

\begin{definición}[Momento Poblacional]
Se define el momento poblacional de orden $k$ respecto al origen como
\[\alpha_{k}=E\left[X^{k}\right]\]
y el momento poblacional de orden $k$ respecto a la media como
\[\beta_{k}=E\left[(X-\mu)^{k}\right]\]
\end{definición}

\begin{observación}
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\vspace{-\topsep} % Removes the default vertical spacing
\begin{itemize}
	\item El momento poblacional de orden 1 respecto al origen es la media poblacional
	      $(\alpha_1 = \mu)$.
	\item El momento poblacional de orden 2 respecto a la media es la varianza
	      poblacional $(\beta_2 = \sigma^2)$.
\end{itemize}
\end{observación}

\begin{proposición}[Propiedades asintóticas de los momentos muestrales]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ m.a.s.$(n)$ de $X \sim N(\mu, \sigma)$, entonces se cumple que:
\begin{enumerate}
	\item Momentos muestrales respecto al origen:
	      \begin{enumerate}
		      \item $$    a_{k}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{k} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \alpha_{k}=E\left[X^{k}\right]$$
		      \item $$     \bar{X} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \mu \quad \text{(Ley Fuerte de Kintchine, } \mu<\infty \text{)}$$
	      \end{enumerate}
	\item Momentos muestrales respecto a la esperanza/media:
	      \begin{enumerate}
		      \item  $$b_{k}=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\bar{X}\right)^{k} \xrightarrow[n \rightarrow \infty]{\text{c.s.}} \beta_{k}=E\left[(X-\mu)^{k}\right]$$
		      \item $$ \sigma_n^2 = \frac{1}{n}\sum_{i = 1}^{n}(X_i - \bar{X})^2 \underset{n \to \infty}{\xrightarrow{c.s.}} \sigma^2 $$
		      \item $$ S_n^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(X_i - \bar{X})^2\underset{n \to \infty}{\xrightarrow{c.s.}} \sigma^2 $$
	      \end{enumerate}
\end{enumerate}
\end{proposición}


\textbf{Propiedades asintóticas de los momentos muestrales}\\

\[
	\sqrt{n} \frac{a_k-\alpha_k}{\sqrt{a_{2k}-a_k^2}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\[
	\sqrt{n} \frac{\bar{X}-\mu}{\sigma} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1), \quad \sigma=\sqrt{E\left[X^{2}\right]-\mu^{2}}
\]
\[
	\text{(Teorema Central del Límite de Levy-Lindeberg, } \mu<\infty, \sigma<\infty \text{)}
\]

\[
	\sqrt{n} \frac{b_{k}-\beta_{k}}{\sqrt{\beta_{2k}-\beta_{k}^{2}-2k \beta_{k-1} \beta_{k+1}+k^{2} \beta_{k-1}^{2} \beta_{2}}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\[
	\sqrt{n} \frac{\sigma_n^2-\sigma^2}{\sqrt{\beta^4-\sigma^4}} \xrightarrow[n \rightarrow \infty]{d} Z \sim N(0,1)
\]

\subsection{Resultados de convergencias}

\begin{teorema}[de Slutsky]
	Si $X_{n} \xrightarrow[n \rightarrow \infty]{d} X$ y $Y_{n} \xrightarrow[n \rightarrow \infty]{P}$ a entonces
	\begin{enumerate}
		\item $Y_{n} X_{n} \xrightarrow[n \rightarrow \infty]{d} a X$
		\item $X_{n}+Y_{n} \xrightarrow[n \rightarrow \infty]{d} X+a$
		\item $\frac{X_{n}}{Y_{n}} \xrightarrow[n \rightarrow \infty]{d} \frac{X}{a}$ siempre que $a \neq 0$
	\end{enumerate}
\end{teorema}

\begin{lema}
	Si $\left\{a_{n}\right\}$ es una sucesión de constantes con $\lim _{n \rightarrow \infty} a_{n}=+\infty$ y $a$ es un número fijo tal que
	\[a_{n}\left(X_{n}-a\right) \xrightarrow[n \rightarrow \infty]{d} X\]
	entonces para cualquier función $g: \mathbb{R} \to \mathbb{R}$ con derivada
	continua y no nula en a se tiene que
	$a_{n}\left(g\left(X_{n}\right)-g(a)\right) \underset{n \rightarrow
			\infty}{\stackrel{d}{\longrightarrow}} g^{\prime}(a) X$
\end{lema}

\ejemplo{
Sea $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$, se pide calcular la distribución de la media muestral:\\
Tenemos que $\varphi_{X}(t)=e^{i t \mu-\frac{1}{2} t^{2} \sigma^{2}} \implies$\\
$E\left[e^{\bar{X}}\right] = \varphi_{\bar{X}}(t)=E\left[e^{it \frac{1}{n} \sum_{i=1}^{n}
				X_{i}}\right]=\varphi_{\sum_{i=1}^{n}
		X_{i}}\left(\frac{t}{n}\right)=\left(\varphi_{X}\left(\frac{t}{n}\right)\right)^{n}=e^{i
	t \mu-\frac{1}{2} \frac{\sigma^{2}}{n} t^{2}}$.\\ Por lo tanto $\bar{X}
	\sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right)$
}
\leavevmode

\ejemplo{
	Sea \( X_1, \dots, X_n \) una muestra aleatoria simple de una variable aleatoria \( X \sim N(0, \sigma) \). La función de densidad de \( X \) es:

	\[
		f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2\sigma^2} x^2}
	\]

	Calculemos la distribución de \( a_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 \)

	Definimos la variable estandarizada:

	\[
		Z_i = \frac{X_i}{\sigma} \sim N(0,1).
	\]

	Entonces, la suma de los cuadrados sigue una distribución chi-cuadrado:

	\[
		\sum_{i=1}^{n} Z_i^2 \sim \chi_n^2.
	\]

	La distribución chi-cuadrado con \( n \) grados de libertad es un caso particular de la distribución Gamma:

	\[
		\chi_n^2 \sim \text{Gamma} \left( a = \frac{1}{2}, p = \frac{n}{2} \right).
	\]

	Donde:
	- \( a = \frac{1}{2} \) es el **parámetro de forma**.
	- \( p = \frac{n}{2} \) es el **parámetro de escala**.

	La función de densidad de la suma de cuadrados es:

	\[
		f_{\sum_{i=1}^{n} Z_i^2}(y) = \frac{1}{2^{n/2} \Gamma(n/2)} y^{n/2-1} e^{-y/2}.
	\]

	Como \( X_i^2 = \sigma^2 Z_i^2 \), al tomar la media muestral obtenemos:

	\[
		a_2 = \frac{1}{n} \sum_{i=1}^{n} X_i^2 = \sigma^2 \frac{1}{n} \sum_{i=1}^{n} Z_i^2.
	\]

	Sustituyendo la distribución Gamma de la suma de \( Z_i^2 \), se tiene:

	\[
		a_2 \sim \text{Gamma} \left( a = \frac{n}{2\sigma^2}, p = \frac{n}{2} \right).
	\]

	Para muestras grandes, usando el **Teorema Central del Límite**, la variable estandarizada:

	\[
		\sigma^2 \sqrt{\frac{n}{2\sigma^2}} (a_2 - \sigma^2)
	\]

	converge en distribución a una normal estándar:

	\[
		N(0,1).
	\]

	Este resultado es fundamental en inferencia estadística, ya que muestra que la varianza muestral puede aproximarse por una normal para muestras grandes.

}

\begin{lema}
	Si \( Y \sim \operatorname{Gamma}(a, p) \), entonces \( T = 2a Y \sim \operatorname{Gamma}\left(\frac{1}{2}, p\right) \).
\end{lema}

\begin{proof}
	Sabemos que la función de densidad de probabilidad de una variable aleatoria \( Y \) que sigue una distribución Gamma con parámetros \( a \) y \( p \) es:

	\[
		f_{Y}(y) = \frac{a^{p}}{\Gamma(p)} e^{-a y} y^{p-1}, \quad y \geq 0.
	\]

	Ahora, definimos la variable \( T \) como \( T = 2a Y \), lo que implica que \(
	Y = \frac{1}{2a} T \). Usamos el cambio de variable para encontrar la función
	de densidad de probabilidad de \( T \). El jacobiano de este cambio es:

	\[
		J = \left| \frac{dY}{dT} \right| = \frac{1}{2a}.
	\]

	Por lo tanto, la función de densidad de probabilidad de \( T \) se obtiene
	sustituyendo en la fórmula general para el cambio de variable:

	\[
		f_{T}(t) = f_{Y}\left(\frac{1}{2a} t\right) \cdot \frac{1}{2a}.
	\]

	Sustituyendo la expresión de \( f_Y(y) \), obtenemos:

	\[
		f_{T}(t) = \frac{a^{p}}{\Gamma(p)} e^{-a \left(\frac{1}{2a} t\right)} \left( \frac{1}{2a} t \right)^{p-1} \cdot \frac{1}{2a}.
	\]

	Simplificando, tenemos:

	\[
		f_{T}(t) = \frac{\left( \frac{1}{2} \right)^{p}}{\Gamma(p)} e^{-\frac{1}{2} t} t^{p-1}, \quad t \geq 0.
	\]

	Esta es precisamente la función de densidad de una distribución Gamma con
	parámetros \( \left( \frac{1}{2}, p \right) \), lo que demuestra que \( T \sim
	\operatorname{Gamma}\left( \frac{1}{2}, p \right) \).
\end{proof}

\ejemplo{
Sea $\left(X_{1}, \dots, X_{n}\right)$ una m.a.s.(n) de $X \sim N(0, \sigma)$.
Entonces, la función de densidad es:

\[
	f(x) = \frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{x^2}{2\sigma^{2}}}
\]

Se pide calcular la distribución de

\[
	a_{2}=\frac{1}{n} \sum_{i=1}^{n} X_{i}^{2}
\]

Tipificando las variables aleatorias \( X_i \) como:

\[
	Z_{i}=\frac{X_{i}}{\sigma} \sim N(0,1),
\]

se tiene que

\[
	\sum_{i=1}^{n} Z_{i}^{2} \sim \chi_{n}^{2} \equiv \operatorname{Gamma} \left(a=\frac{1}{2}, p=\frac{n}{2} \right)
\]

La función de densidad de esta suma es:

\[
	f_{\sum_{i=1}^{n} Z_{i}^{2}}(y) = \frac{1}{2^{\frac{n}{2}} \Gamma\left(\frac{n}{2}\right)} e^{-\frac{y}{2}} y^{\frac{n}{2}-1}
\]

Dado que

\[
	a_{2} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} = \frac{\sigma^{2}}{n} \sum_{i=1}^{n} Z_{i}^{2},
\]

definimos el cambio de variable

\[
	J = \frac{n}{\sigma^{2}}.
\]

Por lo tanto, la función de densidad de $a_2$ es:

\[
	f_{a_{2}}(t) = f_{\sum_{i=1}^{n} Z_{i}^{2}} \left( \frac{n}{\sigma^{2}} t \right) \frac{n}{\sigma^{2}} = \frac{\left(\frac{n}{2\sigma^{2}}\right)^{\frac{n}{2}}}{\Gamma\left(\frac{n}{2}\right)} e^{-\frac{n}{2\sigma^{2}} t} t^{\frac{n}{2}-1}.
\]

Por lo tanto,

\[
	a_{2} = \frac{1}{n} \sum_{i=1}^{n} X_{i}^{2} \sim \operatorname{Gamma} \left(a=\frac{2\sigma^{2}}{n} , p=\frac{n}{2}\right).
\]

Finalmente, bajo el límite

\[
	\sigma^{2} \sqrt{\frac{n}{2}}\left(a_{2}-\sigma^{2}\right) \xrightarrow{d} N(0,1) \quad \text{cuando } n \to \infty.
\]
}

\begin{teorema}[de Fisher]
	Sea $\left(X_{1}, \dots, X_{n}\right)$ una m.a.s. $(n)$ de $X \sim N(\mu, \sigma)$, con $\mu$ y $\sigma$ desconocidos. Se cumple que:

	\begin{enumerate}

		\item La media muestral y la varianza muestral:

		      \[
			      \bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_{i}, \quad S^{2} = \frac{1}{n-1} \sum_{i=1}^{n} \left(X_{i} - \bar{X} \right)^{2}
		      \]

		      son variables aleatorias independientes.

		\item Sus distribuciones son:

		      \[
			      \bar{X} \sim N\left(\mu, \frac{\sigma}{\sqrt{n}}\right),
		      \]

		      \[
			      \frac{(n-1) S^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}.
		      \]

		\item La siguiente variable aleatoria sigue una distribución t de Student:

		      \[
			      \frac{\bar{X} - \mu}{\frac{S}{\sqrt{n}}} \sim t_{n-1}.
		      \]

	\end{enumerate}
\end{teorema}

\begin{proof}
	\leavevmode
	\begin{enumerate}
		%Demostración del primer punto%
		\item $$ \bar{X} = \frac{1}{n}\sum_{i = 1}^{n}X_i \text{ y } S^2 = \frac{1}{n-1}\sum_{i = 1}^{n}(X_i -\bar{X})^2 \text{ son independientes} \iff X_i - \bar{X} \text{ y } \bar{X} \text{ son independientes}$$
		      $ \implies$ Veamos la independencia a través de la covarianza: $Cov(X_i -\bar{X}, \bar{X}) = 0$?\\
		      $$ Cov(X_i - \bar{X}, \bar{X}) = E[(X_i - \bar{X})\bar{X}] - E[X_i- \bar{X}]E[\bar{X}] = E[(X_i - \bar{X}\bar{X})] - (E[X_i] - E[\bar{X}])E[\bar{X}] = $$ $$ = E[(X_i - \bar{X})\bar{X}] - (\mu - \mu)\mu = E[X_i \cdot \bar{X}] - E[\bar{X}^2] = E[X_i\cdot \bar{X}] - (V[X]+ E[\bar{X}]^2) = E[X_i \cdot \bar{X}] - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = E[\frac{1}{n}X_1\cdot X_i + \ldots + \frac{1}{n}X_i^2 + \ldots + \frac{1}{n}X_n\cdot X_i] - (\frac{\sigma^2}{n} - \mu^2) = $$ $$ = \frac{1}{n}(E[X_1X_i] + \ldots + E[X_nX_i]) +\frac{1}{n}E[X_i^2] - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = \frac{n-1}{n}(\mu^2) + \frac{1}{n}(\sigma^2 + \mu^2) - (\frac{\sigma^2}{n} + \mu^2) = $$ $$ = \frac{n-1}{n}\mu^2 + \frac{\mu^2}{n} - \mu^2 + \frac{\sigma^2}{n} - \frac{\sigma^2}{n} = 0 \implies$$ $$ Cov(X_i - \bar{X}, \bar{X}) = 0 \implies \text{son independientes}$$.\\

		      %Demostracion del segundo punto%
		\item Denotemos por $S_{n+1}^{2}=\frac{1}{n} \sum_{i=1}^{n+1}\left(X_{i}-\bar{X}_{n+1}\right)^{2}$\\
		      Demostremos que $n S_{n+1}^{2}=(n-1) S_{n}^{2}+\left(X_{n+1}-\bar{X}_{n}\right)^{2} \frac{n}{n+1}$\\
		      $n S_{n+1}^{2}=\sum_{i=1}^{n+1}\left(X_{i}-\bar{X}_{n+1}^{2}\right)=\sum_{i=1}^{n}\left(\left(X_{i}-\bar{X}_{n}\right)+\left(\bar{X}_{n}-\bar{X}_{n+1}\right)\right)^{2}+\left(X_{n+1}-\bar{X}_{n+1}\right)^{2}=$\\
		      $(n-1) S_{n}^{2}+n\left(\bar{X}_{n}-\bar{X}_{n+1}\right)^{2}+\left(X_{n+1}-\bar{X}_{n+1}\right)^{2} + \sum_{i=0}^{n}2\left(X_{i}-\bar{X}_{n}\right)\left(\bar{X}_{n}-\bar{X}_{n+1}\right)=$\\
		      $(n-1) S_{n}^{2}+n\left(\bar{X}_{n}-\bar{X}_{n+1}\right)^{2}+\left(X_{n+1}-\bar{X}_{n+1}\right)^{2} + 2\left(\bar{X}_{n}-\bar{X}_{n+1}\right)\sum_{i=0}^{n}\left(X_{i}-\bar{X}_{n}\right)=$\\
		      $(n-1) S_{n}^{2}+n\left(\bar{X}_{n}-\bar{X}_{n+1}\right)^{2}+\left(X_{n+1}-\bar{X}_{n+1}\right)^{2}=$\\
		      En seste ultimo paso se desarrollan cuadrados, y se aplica la definicion de $\bar{X}_{n} = \frac{1}{n} \sum_{i=1}^{n} X_{i} \leftrightarrow n \bar{X}_{n} = \sum_{i=1}^{n} X_{i}$\\
		      Ahora si se aplica que $\bar{X}_{n+1}=\frac{n \bar{X}_{n}+X_{n+1}}{n+1} \leftrightarrow \left(X_{n+1}-\bar{X}_{n+1}\right) = \frac{(n+1)X_{n+1}-n\bar{X}_{n}-X_{n+1}}{n+1}$\\
		      $(n-1) S_{n}^{2}+\left(X_{n+1}-\bar{X}_{n}\right)^{2} \frac{n}{(n+1)^2}+\left(X_{n+1}-\bar{X}_{n}\right)^{2} \frac{n^2}{(n+1)^2}=$\\
		      $(n-1) S_{n}^{2}+\left(X_{n+1}-\bar{X}_{n}\right)^{2} \frac{n}{n+1}$\\
		      Asi ya tenemos el resultado deseado

		      Ahora veamos: $\left(\frac{X_{n+1} -\bar{X}_n}{\sigma\sqrt{\frac{n+1}{n}}}\right)^2 \sim \chi_1^2$\\
		      \begin{align}
			      Usemos: X_{n+1} \sim N(\mu, \sigma) \text{ y } \bar{X}_n \sim N(\mu, \frac{\sigma}{\sqrt{n}}) \\
			      X_{n+1} - \bar{X}_n \sim N(0, \sigma \sqrt{\frac{n+1}{n}})                                    \\
			      \left(\frac{X_{n+1} - \bar{X}_n}{\sigma\sqrt{\frac{n+1}{n}}}\right) \sim N(0,1) \sim \chi_1^2
		      \end{align}
		      Finalmente apliquemos induccion:
		      Nuestro caso base es el siguiente:
		      $\frac{1S_{2}^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}\left(\left(X_{1}-\bar{X}_{2}\right)^{2}+\left(X_{2}-\bar{X}_{2}\right)^{2}\right)=\frac{1}{\sigma^{2}}\left(\left(X_{1}-\frac{X_1+X_2}{2}\right)^{2}+\left(X_{2}-\frac{X_1+X_2}{2}\right)^{2}\right)=$\\
		      $\frac{1}{\sigma^{2}}\left(\frac{1}{4}\left(X_{1}-X_{2}\right)^{2}+\frac{1}{4}\left(X_{2}-X_{1}\right)^{2}\right)=\frac{1}{\sigma^{2}}\frac{1}{2}\left(X_{2}-\bar{X_{1}}\right)^{2}=$\\
		      $\left(\frac{\left(X_{2}-\bar{X_{1}}\right)}{\sigma\sqrt{2}}\right)^{2} \sim \chi_{1}^{2}$\\ Por lo visto antes
		      Nuestra hipotesis de induccion es que $\frac{(n-1) S_{n}^{2}}{\sigma^{2}} \sim \chi_{n-1}^{2}$\\
		      Asi, por inducción (teniendo en cuenta que $S_{n}^{2}$ y $\bar{X}_{n}$ son independientes por (1))\\
		      $\frac{nS_{n+1}^{2}}{\sigma^{2}} = \frac{(n-1) S_{n}^{2}}{\sigma^{2}}+\left(\frac{X_{n+1}-\bar{X}_{n}}{\sigma \sqrt{\frac{n+1}{n}}}\right)^{2} \sim \chi_{n-1}^{2} + \chi_{1}^{2} \sim \chi_{n}^{2}$

		      % Demostración del tercer punto%
		\item Se deduce de los anteriores
	\end{enumerate}
\end{proof}


\begin{definición}[Estadísticos ordenados]
Sea $\left(X_{1}, \ldots, X_{n}\right)$ una m.a.s. $(n)$ de $X$. Podemos ordenar los valore de menor a mayor. A éstos se les llama estadísticos ordenados y se denotan por $X_{(1)} \leq X_{(2)} \leq \ldots \leq X_{(n)}$.
Sus funciones de ditribución y densidad son:
$$F_{X_{(n)}}(x)=F(x)^{n} \implies f_{X_{(n)}}(x)= \frac{\partial}{\partial{x}}(F(x)^n) = n F(x)^{n-1} f(x)$$
$$F_{X_{(1)}}(x)=1-(1-F(x))^{n} \implies f_{X_{(1)}}(x)= \frac{\partial}{\partial{x}}(1-(1-F(x))^n) = n(1-F(x))^{n-1} f(x)$$
$$F_{X_{(r)}}(x) = P\left(\sum_{i=1}^{n} I_{(-\infty, x]}(X_i) \geq r\right) = P(\operatorname{Bin}(n, F(x)) \geq r) = \sum_{j=r}^{n} \binom{n}{j} F(x)^j(1-F(x))^{n-j} \implies$$  $$\\ f_{X_{(r)}}(x) = \binom{n}{r} r F(x)^{r-1}(1-F(x))^{n-r} f(x)$$
$$f_{\left(X_{(r)}, X_{(s)}\right)}(x, y) = \frac{n!}{(r-1)!(s-r-1)!(n-s)!} F(x)^{r-1}(F(y)-F(x))^{s-r-1}(1-F(y))^{n-s} f(x) f(y)$$
$$f_{\left(X_{(1)}, \ldots, X_{(n)}\right)}(y_1, \ldots, y_n) = n! \prod_{j=1}^{n} f(y_j) \text{ si } y_1 < \ldots < y_n$$
\end{definición}

\ejemplo{
	Sea $X_1, X_2, \dots, X_n$ una muestra aleatoria simple de una distribución uniforme en $(0,1)$, es decir:

	$$ X_1, X_2, \dots, X_n \overset{i.i.d}{\sim} U(0,1) $$

	La función de distribución acumulada de una variable uniforme en $(0,1)$ es:

	$$ F(x) =
		\begin{cases}
			0, & x \leq 0  \\
			x, & 0 < x < 1 \\
			1, & x \geq 1
		\end{cases} $$

	Ordenando la muestra de menor a mayor, los estadísticos ordenados se denotan como $X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n)}$. Se quiere encontrar la función de densidad de estos valores ordenados.

	Para el máximo, $X_{(n)}$, se tiene que su función de distribución es:

	$$ P(X_{(n)} \leq x) = P(X_1 \leq x, X_2 \leq x, \dots, X_n \leq x) $$

	Dado que los datos son independientes, esto se factoriza como:

	$$ P(X_{(n)} \leq x) = F(x)^n = x^n, \quad 0 < x < 1 $$

	Derivando se obtiene la función de densidad:

	$$ f_{X_{(n)}}(x) = \frac{d}{dx} x^n = n x^{n-1}, \quad 0 < x < 1 $$

	Es decir, $X_{(n)} \sim Beta(n,1)$.

	Para el mínimo, $X_{(1)}$, la función de distribución se obtiene como:

	$$ P(X_{(1)} \leq x) = 1 - P(X_{(1)} > x) = 1 - P(X_1 > x, X_2 > x, \dots, X_n > x) $$

	Por independencia,

	$$ P(X_1 > x, X_2 > x, \dots, X_n > x) = (1 - F(x))^n = (1-x)^n $$

	Por lo que,

	$$ P(X_{(1)} \leq x) = 1 - (1-x)^n, \quad 0 < x < 1 $$

	Derivando se obtiene la densidad:

	$$ f_{X_{(1)}}(x) = n (1-x)^{n-1}, \quad 0 < x < 1 $$

	De manera general, para el $r$-ésimo estadístico ordenado, su densidad es:

	$$ f_{X_{(r)}}(x) = \frac{n!}{(r-1)! (n-r)!} x^{r-1} (1-x)^{n-r}, \quad 0 < x < 1 $$

	lo que implica que $X_{(r)} \sim Beta(r, n-r+1)$.

	La densidad conjunta del mínimo y el máximo de la muestra es:

	$$ f_{X_{(1)}, X_{(n)}}(x,y) = n(n-1)(y-x)^{n-2}, \quad 0 < x < y < 1 $$

	Esto muestra cómo la distribución de los estadísticos ordenados sigue distribuciones beta en función de la posición del orden estadístico dentro de la muestra.

}


\ejemplo{
Calcular la distribución del rango muestral

$$
	\left.\left.\begin{array}{l}
		R=X_{(n)}-X_{(1)} \\
		H=\frac{X_{(n)}+X_{(1)}}{2}
	\end{array}\right\} \begin{array}{c}
		X_{(n)}=H+\frac{R}{2} \\
		X_{(1)}=H-\frac{R}{2}
	\end{array}\right\} J=\left|\begin{array}{cc}
		1 & -\frac{1}{2} \\
		1 & \frac{1}{2}
	\end{array}\right|=1 .
$$

\[
	0 < x < y < 1 \Rightarrow \frac{r}{2} < h < 1 - \frac{r}{2}
\]

\[
	g_{(R,H)}(r,h) = f_{(X_{(1)},X_{(n)})} \left(h - \frac{r}{2}, h + \frac{r}{2} \right) = 1 - n(n-1) r^{n-2}
\]

\[
	g_R(r) = n(n-1) r^{n-2} (1 - r) \Rightarrow R \sim \operatorname{Beta}(n-1,2)
\]

}

\vspace{1em} % Adds vertical space of 1em

\ejemplo{
	Sea $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$ tal que $F(x)=P(X \leq x)$\\
	Se pide calcular la distribución de $U=F(X)$ y de $U_{R}=F\left(X_{(r)}\right)$\\
	$G_{U}(u)=P(U \leq u)=P(F(X) \leq u)=P\left(X \leq F^{-1}(u)\right)=F\left(F^{-1}(u)\right)=u, \quad 0<u<1$\\
	$\Rightarrow U=F(X) \sim U(0,1)$\\
	$G_{U_{R}}(u)=P\left(U_{R} \leq u\right)=P\left(F\left(X_{(r)}\right) \leq u\right)=P\left(X_{(r)} \leq F^{-1}(u)\right)=$\\
	$F_{X_{(r)}}\left(F^{-1}(u)\right)=\sum_{j=r}^{n}\binom{n}{j} F\left(F^{-1}(u)\right)^{j}\left(1-F\left(F^{-1}(u)\right)\right)^{n-j}=\sum_{j=r}^{n}\binom{n}{j} u^{j}(1-u)^{n-j}$\\
	$\Rightarrow U_{R}=F\left(X_{(r)}\right)$ es el estadístico ordenado de orden $r$ asociado a una m.a.s. $(n)$ de una población $U=F(X) \sim U(0,1)$
}
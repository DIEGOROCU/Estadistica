\section{Reducción de Datos}

\begin{definición}[Estadístico suficiente]
  Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado ( $\left.\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$\\
  $T=T\left(X_{1}, \cdots X_{n}\right): \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}$ es un estadístico suficiente para $\theta$ (para la familia de funciones de distribución $\left\{F_{\theta}\right\}_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$ ), sí y sólo sí la distribución de probabilidad de la muestra condicionada por $T$ es independiente de $\theta$
  \end{definición}
  
  \ejemplo{
  Demostrar que si $X \sim \operatorname{Bin}(1, \theta)$, entonces $T=\sum_{i=1}^{n} X_{i} \sim \operatorname{Bin}(n, \theta)$ es suficiente para $\theta$\\
  $P_{\theta}(X=x)=\left.\theta^{x}(1-\theta)^{1-x}\right|_{\{0,1\}}(x)$\\
  $P_{\theta}\left(\sum_{i=1}^{n} X_{i}=t\right)=\binom{n}{t} \theta^{t}(1-\theta)^{n-t}\left\{_{\{0,1, \ldots, n\}}(t)\right.$\\
  $P_{\theta}\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n}\right)=\prod_{i=1}^{n} \theta^{x_{i}}(1-\theta)^{1-x_{i}} I_{\{0,1\}}\left(x_{i}\right)=$\\
  $\theta^{\sum_{i=1}^{n} x_{i}}(1-\theta)^{n-\sum_{i=1}^{n} x_{i}} \prod_{i=1}^{n} l_{\{0,1\}}\left(x_{i}\right)$\\
  Si $\sum_{i=1}^{n} x_{i}=t, P_{\theta}\left(X_{1}=x_{1}, \ldots, X_{n}=x_{n} \mid \sum_{i=1}^{n} X_{i}=t\right)=$\\
  $\frac{P_{\theta}\left(X_{1}=x_{1}, \ldots, x_{n-1}=x_{n-1}, X_{n}=t-\sum_{i=1}^{n-1} x_{i}\right)}{P\left(\sum_{i=1}^{n} X_{i}=t\right)}=\frac{\theta^{t}(1-\theta)^{n-t}}{\left(\begin{array}{l}n \\ t\end{array} \theta^{t}(1-\theta)^{n-t}\right.}=\frac{1}{\binom{n}{t}}, t=0,1, \ldots, n$
  }
  
  
  \subsection*{Teorema de Factorización de Fisher}
  
  \begin{teorema}[de Factorización de Fisher]
    Teorema de Factorización (caracterización de estadísticos suficientes) $T=T\left(X_{1}, \cdots X_{n}\right): \mathbb{R}^{n} \longrightarrow \mathbb{R}^{m}$ es un estadístico suficiente para $\theta$ sí y sólo sí existen funciones reales positivas $h: \mathbb{R}^{n} \longrightarrow \mathbb{R}$ y $g_{\theta}: \mathbb{R}^{m} \longrightarrow \mathbb{R}$ tales que $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}\left(T\left(x_{1}, \ldots, x_{n}\right)\right)$, donde $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)$ es la función de densidad o de masa de la muestra
  \end{teorema}
  
  
  \begin{proof}
    \leavevmode
    $\Leftrightarrow)$ Si $T\left(x_{1}, \ldots, x_{n}\right)=t$,\\
    $f_{\theta}\left(x_{1}, \ldots, x_{n} \mid t\right)=\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}(t)}=\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{\sum_{\left\{\left(y_{1}, \ldots, y_{n}\right): T\left(y_{1}, \ldots, y_{n}\right)=t\right\}} f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}=$\\
    $\frac{h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}(t)}{\sum_{\left\{\left(y_{1}, \ldots, y_{n}\right): T\left(y_{1}, \ldots, y_{n}\right)=t\right\}} h\left(y_{1}, \ldots, y_{n}\right) g_{\theta}(t)}=\frac{h\left(x_{1}, \ldots, x_{n}\right)}{\left.\sum_{\left.\left\{y_{1}, \ldots, y_{n}\right): T\left(y_{1}, \ldots, y_{n}\right)=t\right\}}\right\}\left(y_{1}, \ldots, y_{n}\right)}$ es\\
    independiente de $\theta \Rightarrow T$ es suficiente para $\theta$\\
    $\Rightarrow)$ Sea $T=T\left(X_{1}, \cdots X_{n}\right)$ un estadístico suficiente para $\theta \Rightarrow$ $f_{\theta}\left(x_{1}, \ldots, x_{n} \mid t\right)$ es independiente de $\theta$\\
    Si $T\left(x_{1}, \ldots, x_{n}\right)=t, f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=f_{\theta}\left(x_{1}, \ldots, x_{n} \mid t\right) f_{\theta}(t)=$ $h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}\left(T\left(x_{1}, \ldots, x_{n}\right)\right)$
  \end{proof}
  
  
  
  \section*{Teorema de Factorización de Fisher (continuación)}
  Ejercicio Encontrar un estadísico suficiente para $\theta$ si $X \sim \operatorname{Bin}(1, \theta)$ $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=\prod_{i=1}^{n} \theta^{x_{i}}(1-\theta)^{1-x_{i}} I_{\{0,1\}}\left(x_{i}\right)=$ $\theta^{\sum_{i=1}^{n} x_{i}}(1-\theta)^{n-\sum_{i=1}^{n} x_{i}} \prod_{i=1}^{n} I_{\{0,1\}}\left(x_{i}\right)=g_{\theta}\left(\sum_{i=1}^{n} x_{i}\right) h\left(x_{1}, \ldots, x_{n}\right) \Rightarrow$ $T=\sum_{i=1}^{n} X_{i}$ es suficiente para $\theta$
  
  \section*{Ejercicios propuestos}
  $1 T=\sum_{i=1}^{n} X_{i}$ es suficiente para $\theta$ si $X \sim \operatorname{Poisson}(\theta)$\\
  2 La propia muestra $\left(X_{1}, \ldots, X_{n}\right)$ y el estadístico ordenado $\left(X_{(1)}, \ldots, X_{(n)}\right)$ son suficientes para $\theta$\\
  3 Si $T=T\left(X_{1}, \cdots X_{n}\right)$ es suficiente para $\theta$ entonces cualquier biyección $S=S(T)$ también es suficiente para $\theta$\\
  4 Si $T=T\left(X_{1}, \cdots X_{n}\right)$ y $S=S\left(X_{1}, \cdots X_{n}\right)$ son dos estadísticos suficientes para $\theta$, entonces también es suficiente para $\theta$ el estadístico ( $T, S$ )
  
  \section*{Teorema de Factorización de Fisher (continuación)}
  $5 T=\bar{X}$ es suficiente para $\mu$ si $X \sim N\left(\mu, \sigma_{0}\right), \sigma_{0}$ conocida\\
  $6 T=\sum_{i=1}^{n}\left(X_{i}-\mu_{0}\right)^{2}$ es suficiente para $\sigma$ si $X \sim N\left(\mu_{0}, \sigma\right), \mu_{0}$ conocida\\
  $7 T=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)$ es suficiente para $\theta=(\mu, \sigma)$ si $X \sim N(\mu, \sigma), \mu$ y $\sigma$ desconocidas $\Rightarrow\left(\bar{X}, S_{n}^{2}\right)$ también es suficiente
  
  8 Los estadísticos $\left(X_{(1)}, X_{(n)}\right)$ y $X_{(n)}$ son suficientes para $\theta$ si $X \sim U(0, \theta)$\\
  9 El estadístico $\left(X_{(1)}, X_{(n)}\right)$ es suficiente para $\theta$ si $X \sim U\left(-\frac{\theta}{2}, \frac{\theta}{2}\right)$
  
  \subsection*{Estadístico minimal suficiente}
  
  Dado un estadístico $T=T\left(X_{1}, \cdots X_{n}\right)$, se define $A_{t}=\left\{\left(x_{1}, \ldots, x_{n}\right) \in \chi^{n}: T\left(x_{1}, \ldots, x_{n}\right)=t\right\}$ (órbita)\\
  $K_{T}=\left\{A_{t}, t \in \mathbb{R}^{m}\right\}$ es una partición disjunta de $\chi^{n} \mathrm{y}$ se denomina partición inducida por el estadístico $T$
  
  Se dice que $K_{T}$ es suficiente sí y sólo si $T$ es suficiente\\
  Dados dos estadísticos $T=T\left(X_{1}, \cdots X_{n}\right)$ y $S=S\left(X_{1}, \cdots X_{n}\right), K_{S}$ es una subpartición de $K_{T}$, sí y sólo sí $\forall B \in K_{S}, \exists A \in K_{T}$ tal que $B \subset A$. En este caso, se dice que $K_{T}$ es una partición menos fina que $K_{S}$
  
  \begin{definición}[Estadístico Minimal Suficiente]
  \vspace{-\baselineskip}
  \vspace{-\baselineskip}
  \begin{enumerate}
    \item $T=T\left(X_{1}, \cdots X_{n}\right)$ es minimal suficiente sí y sólo sí $K_{T}$ es suficiente y $\forall S=S\left(X_{1}, \cdots X_{n}\right)$ suficiente, $K_{S}$ es una subpartición de $K_{T}$\\
    \item $T=T\left(X_{1}, \cdots X_{n}\right)$ es minimal suficiente sí y sólo sí $T$ es suficiente y $\forall S=S\left(X_{1}, \cdots X_{n}\right)$ suficiente, $\exists \psi$ tal que $\psi(S)=T$
  \end{enumerate}
  \end{definición}
  
  \begin{proof}
    $\Rightarrow)$ Sea $S$ suficiente, si $S\left(x_{1}, \ldots, x_{n}\right)=S\left(y_{1}, \ldots, y_{n}\right)=s \Rightarrow$ $\left(x_{1}, \ldots, x_{n}\right),\left(y_{1}, \ldots, y_{n}\right) \in B_{s} \Rightarrow \exists A_{t} \in K_{T}$ tal que $B_{s} \subset A_{t} \Rightarrow$ $T\left(x_{1}, \ldots, x_{n}\right)=T\left(y_{1}, \ldots, y_{n}\right)=t \Rightarrow \exists \psi$ tal que $\psi(S)=T$ y $T$ es suficiente\\
    $\Leftrightarrow)$ Sea $S$ suficiente y $\varphi$ tal que $\psi(S)=T \Rightarrow T$ es suficiente y si $S\left(x_{1}, \ldots, x_{n}\right)=S\left(y_{1}, \ldots, y_{n}\right)=s \Rightarrow T\left(x_{1}, \ldots, x_{n}\right)=$ $\psi\left(S\left(x_{1}, \ldots, x_{n}\right)\right)=\psi(s)=\psi\left(S\left(y_{1}, \ldots, y_{n}\right)\right)=T\left(y_{1}, \ldots, y_{n}\right) \Rightarrow$ $B_{s} \subset A_{\psi(s)} \Rightarrow K_{S}$ es una subpartición de $K_{T}$
  \end{proof}
  
  \section*{Teorema de caracterización de estadísticos}
  minimales suficientesDefinamos la siguiente relación de equivalencia $\left(x_{1}, \ldots, x_{n}\right) R\left(y_{1}, \ldots, y_{n}\right) \Leftrightarrow \frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}$ es independiente de $\theta$ Asignemos a cada clase del conjunto cociente un valor $t$ y definamos el estadístico $T=T\left(X_{1}, \ldots, X_{n}\right)$ tal que $\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}$ es independiente de $\theta$ cuando $T\left(x_{1}, \ldots, x_{n}\right)=t=T\left(y_{1}, \ldots, y_{n}\right)$. Entonces $T$ es minimal suficiente
  
  \section*{Demostración}
  Supongamos que $T$ es suficiente y demostremos que es minimal Sea $S=S\left(X_{1}, \ldots, X_{n}\right)$ suficiente y $\left(x_{1}, \ldots, x_{n}\right),\left(y_{1}, \ldots, y_{n}\right) \in B_{s} \Rightarrow$ $\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}=\frac{h\left(x_{1}, \ldots, x_{n}\right) g_{\theta}(s)}{h\left(y_{1}, \ldots, y_{n}\right) g_{\theta}(s)}=\frac{h\left(x_{1}, \ldots, x_{n}\right)}{h\left(y_{1}, \ldots, y_{n}\right)}$ es independiente de $\theta \Rightarrow \exists t$ tal que $\left(x_{1}, \ldots, x_{n}\right),\left(y_{1}, \ldots, y_{n}\right) \in A_{t} \Rightarrow K_{S}$ es una subpartición de $K_{T}$
  
  \section*{Teorema de caracterización de estadísticos minimales suficientes (continuación)}
  Demotremos ahora que $T$ es suficiente (caso discreto)\\
  Si $T\left(x_{1}, \ldots, x_{n}\right)=t, f_{\theta}\left(x_{1}, \ldots, x_{n} \mid t\right)=\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}(t)}=$\\
  $\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{\sum_{\left\{\left(y_{1}, \ldots, y_{n}\right): T\left(y_{1}, \ldots, y_{n}\right)=t\right\}} f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}=\frac{1}{\sum_{\left(y_{1}, \ldots, y_{n}\right) \in A_{t}} \frac{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}}$ es independiente de $\theta \Rightarrow T$ es suficiente para $\theta$
  
  Ejercicio $\sum_{i=1}^{n} X_{i}$ es minimal suficiente para $\theta$ si $X \sim \operatorname{Bin}(1, \theta)$\\
  $f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=\left.\prod_{i=1}^{n} \theta^{x_{i}}(1-\theta)^{1-x_{i}}\right|_{\{0,1\}}\left(x_{i}\right)=$\\
  $\theta^{\sum_{i=1}^{n} x_{i}}(1-\theta)^{n-\sum_{i=1}^{n=1} x_{i}} \prod_{i=1}^{n} I_{\{0,1\}}\left(x_{i}\right)$\\
  $\frac{f_{\theta}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}=\left(\frac{\theta}{1-\theta}\right)^{\sum_{i=1}^{n} x_{i}-\sum_{i=1}^{n} y_{i}}$ es independiente de $\theta$ cuando\\
  $\sum_{i=1}^{n} x_{i}=\sum_{i=1}^{n} y_{i}$
  
  \subsection*{Familia exponencial $k$-paramétrica}
  
  \begin{definición}[Familia exponencial $k$-paramétrica]
  Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado ( $\left.\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$
  
  La distribución de $X$ pertenece a la familia exponencial $k$-paramétrica sí y sólo sí
  \[f_{\theta}(x)=c(\theta) h(x) e^{\sum_{j=1}^{k} q_{j}(\theta) T_{j}(x)}\]
  \[f_{\theta}\left(x_{1}, \ldots, x_{n}\right)=c(\theta)^{n} \prod_{i=1}^{n} h\left(x_{i}\right) e^{\sum_{j=1}^{k} q_{j}(\theta) \sum_{i=1}^{n} T_{j}\left(x_{i}\right)}\]
  Entonces, $\left(\sum_{i=1}^{n} T_{1}\left(X_{i}\right), \ldots, \sum_{i=1}^{n} T_{k}\left(X_{i}\right)\right)$ es suficiente para $\theta$ (Teorema de factorización) y se le denomina estadístico natural
  \end{definición}
  
  \[X \sim N(\sigma, \mu) \quad f_\theta = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}(x - \mu)^2}\]
  Como $(x- \mu)^2 = x^2 + \mu^2 -2x\mu$\\
  \[f_\theta = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{1}{2\sigma^2}x^2}e^{\frac{\mu}{\sigma^2}x}e^{-\frac{\mu^2}{2\sigma^2}}\]
  
  \section*{Familia exponencial $k$-paramétrica (continuación)}
  Teorema 1 Sean $\theta_{1} \ldots, \theta_{k} \in \Theta \subset \mathbb{R}^{\ell}$ tales que los vectores $c_{r}=\left(q_{1}\left(\theta_{r}\right), \ldots, q_{k}\left(\theta_{r}\right)\right), r=1, \ldots, k$ son linealmente independientes, entonces el estadístico natural suficiente de la familia exponencial $k$-paramétrica es minimal
  
  \section*{Demostración}
  $\frac{f_{f}\left(x_{1}, \ldots, x_{n}\right)}{f_{\theta}\left(y_{1}, \ldots, y_{n}\right)}=\frac{c(\theta)^{n} \prod_{i=1}^{n} h\left(x_{i}\right) \sum^{\sum_{j=1}^{k} q_{j}(\theta) \sum_{i=1}^{n} T_{j}\left(x_{i}\right)}}{c(\theta)^{n} \prod_{i=1}^{n} h\left(y_{i}\right) e^{\sum_{j=1}^{k} q_{j}(\theta) \sum_{i=1}^{n} T_{j}\left(y_{i}\right)}}$\\
  $=\frac{\prod_{i=1}^{n} h\left(x_{i}\right)}{\prod_{i=1}^{i} h\left(y_{i}\right)} e^{\sum_{j=1}^{k} q_{j}(\theta)\left(\sum_{i=1}^{n} T_{j}\left(x_{i}\right)-\sum_{i=1}^{n} T_{j}\left(y_{i}\right)\right)}$ es independiente de $\theta$ sí $y$ sólo sí $\sum_{j=1}^{k} q_{j}(\theta)\left(\sum_{i=1}^{n} T_{j}\left(x_{i}\right)-\sum_{i=1}^{n} T_{j}\left(y_{i}\right)\right)=0$. En este caso, el sistema homogéneo $\sum_{j=1}^{k} q_{j}\left(\theta_{r}\right)\left(\sum_{i=1}^{n} T_{j}\left(x_{i}\right)-\sum_{i=1}^{n} T_{j}\left(y_{i}\right)\right)=0$, $r=1, \ldots, k$, sólo admite la solución $\sum_{i=1}^{n} T_{j}\left(x_{i}\right)-\sum_{i=1}^{n} T_{j}\left(y_{i}\right)=0$, $r=1, \ldots, k$. Entonces $\left(\sum_{i=1}^{n} T_{1}\left(X_{i}\right), \ldots, \sum_{i=1}^{n} T_{k}\left(X_{i}\right)\right)$ es minimal (Teorema de caracterización de estadísticos minimales suficientes)
  
  \section*{Familia exponencial $k$-paramétrica (continuación)}
  Ejercicio $T=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)$ es minimal suficiente para $\theta=(\mu, \sigma)$ si $X \sim N(\mu, \sigma), \mu$ y $\sigma$ desconocidas $\Rightarrow\left(\bar{X}, S_{n}^{2}\right)$ también es minimal suficiente\\
  $N(\mu, \sigma)$ pertenece a la familia exponencial $k$-paramétrica con $k=2$ $f_{\theta}(x)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{1}{2 \sigma^{2}}(x-\mu)^{2}}=c(\theta) h(x) e^{-\frac{1}{2 \sigma^{2}} x^{2}+\frac{\mu}{\sigma^{2}} x}$\\
  $\Rightarrow T=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)$ es natural suficiente para $\theta$\\
  Además, $q_{1}(\theta)=\frac{\mu}{\sigma^{2}}$ y $q_{2}(\theta)=-\frac{1}{2 \sigma^{2}}$ y tomando $\theta_{1}=(0,1)$ y $\theta_{2}=(1,1)$, los vectores $c_{1}=\left(q_{1}\left(\theta_{1}\right), q_{2}\left(\theta_{1}\right)\right)=\left(0,-\frac{1}{2}\right)$ y $c_{2}=\left(q_{1}\left(\theta_{2}\right), q_{2}\left(\theta_{2}\right)\right)=\left(1,-\frac{1}{2}\right)$ son linealmente independientes $\Rightarrow T$ es minimal
  
  \section*{Familia exponencial $k$-paramétrica (continuación)}
  $S_{n}^{2}=\frac{n}{n-1}\left(\sum_{i=1}^{n} X_{i}^{2}-n \bar{X}^{2}\right)$\\
  Denotemos por $(W, Z)=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)$ y\\
  $(F, G)=\left(\bar{X}, S_{n}^{2}\right)=\left(\frac{w}{n}, \frac{n}{n-1}\left(Z-\frac{W^{2}}{n}\right)\right)$\\
  Transformación inversa ( $W=n F, Z=\frac{n-1}{n} G+n F^{2}$ )\\
  $J=\left|\begin{array}{ll}n & 0 \\ 2 n F & \frac{n-1}{n}\end{array}\right|=n-1 \neq 0$ si $n \geq 2$\\
  Por lo tanto existe una transformación biyectiva $\psi_{1}$ tal que $(F, G)=\psi_{1}(W, Z)$ y como $(W, Z)$ es minimal suficiente, $\forall S$ suficiente, existe una transformación $\psi_{2}$ tal que $\psi_{2}(S)=(W, Z)$. Por lo tanto, $\psi_{1} \psi_{2}(S)=\psi_{1}(W, Z)=(F, G)$ y $(F, G)$ es minimal suficiente
  
  \section*{Estadísticos Ancilarios y Completos}
  Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado ( $\left.\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$
  
  El estadístico $U=U\left(X_{1}, \ldots, X_{n}\right)$ es ancilario para $\theta$ si su distribución en el muestreo es independiente de $\theta$
  
  Ejercicio $\mathrm{Si}\left(X_{1}, \ldots, X_{n}\right)$ es una m.a.s.( $n$ ) de $X \sim N\left(\mu, \sigma_{0}\right), \sigma_{0}$ conocida, entonces $U\left(X_{1}, \ldots, X_{n}\right)=X_{1}-X_{2} \sim N\left(0, \sigma_{0} \sqrt{2}\right)$ es un estadístico ancilario para $\mu$
  
  La familia de distribuciones de probabilidad $\left\{G_{\theta}\left(y_{1}, \ldots, y_{m}\right)\right\}_{\theta \in \Theta \subset \mathbb{R}^{e}}$ es completa sí y sólo sí para cualquier función real $h\left(y_{1}, \ldots, y_{m}\right)$ con $h\left(Y_{1}, \ldots, Y_{m}\right)$ medible y tal que $E_{\theta}\left[h\left(Y_{1}, \ldots, Y_{m}\right)\right]=0, \forall \theta \in \Theta$, se sigue que $h\left(Y_{1}, \ldots, Y_{m}\right) \stackrel{C S}{=} 0$
  
  \section*{Estadísticos Ancilarios y Completos (continuación)}
  Ejercicio La familia de distribuciones de probabilidad $\operatorname{Bin}(n, \theta)$ es completa\\
  Sea $Y \sim \operatorname{Bin}(n, \theta)$,\\
  $E_{\theta}[h(Y)]=\sum_{i=1}^{n} h(i)\binom{n}{i} \theta^{i}(1-\theta)^{n-i}=$\\
  $(1-\theta)^{n} \sum_{i=1}^{n} h(i)\binom{n}{i}\left(\frac{\theta}{1-\theta}\right)^{i}=0, \forall \theta \in(0,1)$,\\
  que es un polinomio de grado $n$ en $\frac{\theta}{1-\theta} \in(0, \infty)$, luego para que sea nulo, ha de ser $h(i)=0, \forall i=1, \ldots, n$. Por lo tanto $h(Y) \stackrel{c s}{=} 0$
  
  Ejercicio La familia de distribuciones de probabilidad $N(0, \theta)$ no es completa\\
  Sea $Y \sim N(0, \theta)$ y $h(Y)=Y$, entonces $E_{\theta}[h(Y)]=E_{\theta}[Y]=0$ $\forall \theta>0$, sin embargo $h(Y)=Y$ no es idénticamente nula c.s.
  
  \section*{Estadísticos Ancilarios y Completos (continuación)}
  El estadístico $T=T\left(X_{1}, \ldots, X_{n}\right)$ es completo sí y sólo sí su distribución en el muestreo es una familia de distribuciones de probabilidad completa
  
  Ejercicio Si $X \sim \operatorname{Bin}(1, \theta), T=\sum_{i=1}^{n} X_{i} \sim \operatorname{Bin}(n, \theta)$ es completo\\
  Ejercicio Si $X \sim N(\theta, \theta), T=\left(\sum_{i=1}^{n} X_{i}, \sum_{i=1}^{n} X_{i}^{2}\right)$ no es completo Indicación: $h(T)=\left(2\left(\sum_{i=1}^{n} X_{i}\right)^{2}-(n+1) \sum_{i=1}^{n} X_{i}^{2}\right)$
  
  Ejercicio Si $S=S\left(X_{1}, \ldots, X_{n}\right)$ es suficiente y completo, entonces es minimal suficiente Indicación: Demostrar que si $T=T\left(X_{1}, \ldots, X_{n}\right)$ es minimal suficiente, entonces $S \stackrel{\text { cs }}{=} E[S \mid T]$ y por lo tanto $S$ es función de $T$
  
  \section*{Estadísticos Ancilarios y Completos (continuación)}
  Teorema 2 El estadístico natural $\left(\sum_{i=1}^{n} T_{1}\left(X_{i}\right), \ldots, \sum_{i=1}^{n} T_{k}\left(X_{i}\right)\right)$ de la familia de distribuciones exponencial $k$-paramétrica, $\left\{f_{\theta}(x)=c(\theta) h(x) e^{\sum_{j=1}^{k} q_{j}(\theta) T_{j}(x)}\right\}_{\theta \in \Theta \subset \mathbb{R}^{e}}$, es completo si la imagen de la aplicación $q=\left(q_{1}(\theta), \ldots, q_{k}(\theta)\right): \Theta \longrightarrow \mathbb{R}^{k}$ contiene un rectángulo abierto de $\mathbb{R}^{k}$
  
  Observación Si $X \sim N(\theta, \theta), q=\left(q_{1}(\theta), q_{2}(\theta)\right)=\left(\frac{1}{\theta},-\frac{1}{2 \theta^{2}}\right) \Rightarrow$ $q_{2}(\theta)=-\frac{1}{2} q_{1}(\theta)^{2}, \forall \theta>0$, que es una rama de parábola, y que por lo tanto no contiene ningún abierto de $\mathbb{R}^{2}$
  
  \section*{Estadísticos Ancilarios y Completos (continuación)}
  Teorema de Basu Si $T=T\left(X_{1}, \ldots, X_{n}\right)$ es un estadístico suficiente y completo y $U=U\left(X_{1}, \ldots, X_{n}\right)$ es un estadístico ancilario, entonces $T$ y $U$ son independientes
  
  Demostración $T=T\left(X_{1}, \ldots, X_{n}\right)$ es un estadístico suficiente $\Rightarrow$ $f\left(x_{1}, \ldots, x_{n} \mid t\right)$ es independiente de $\theta \Rightarrow$ $f(u \mid t)=\sum_{\left(x_{1}, \ldots, x_{n}\right): U\left(x_{1}, \ldots, x_{n}\right)=u} f\left(x_{1}, \ldots, x_{n} \mid t\right)$ es independiente de $\theta$ Además, como $T=T\left(X_{1}, \ldots, X_{n}\right)$ es un estadístico completo y la función $h(t)=f(u \mid t)-f(u)$ tiene media $0, \forall \theta$, respecto a la distribución de $T \sim f_{\theta}(t) \Rightarrow f(u \mid t) \stackrel{c s}{=} f(u)$
  
  Ejercicio Si $X \sim U(0, \theta)$, entonces $X_{(n)}$ y $\frac{X_{(1)}}{X_{(n)}}$ son independientes
  
  \section*{Principios de reducción de datos}
  Sea $(\Omega, \mathcal{A}, \mathcal{P})$ el espacio probabilístico asociado a un experimento aleatorio $\mathcal{E}$, una variable aleatoria observable $X: \Omega \longrightarrow \mathbb{R}$ y su modelo estadístico asociado $\left(\chi, \mathcal{B}, F_{\theta}\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, \mathcal{B}=\mathcal{B}(\mathbb{R})$, y $\left(X_{1}, \cdots X_{n}\right)$ m.a.s. $(n) \sim X$
  
  \section*{Principio de verosimilitud}
  La idea es considerar la distribución de probabilidad de la muestra, no como función de $\left(x_{1}, \ldots, x_{n}\right)$ sino como función del parámetro $\theta$ desconocido
  
  Supuesto que se ha observado un valor muestral $\left(x_{1}, \ldots, x_{n}\right)$, la función de $\theta$ definida mediante $L\left(\theta \mid x_{1}, \ldots, x_{n}\right)=f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$, se llama función de verosimilitud, siendo $f\left(x_{1}, \ldots, x_{n} \mid \theta\right)$ la función de densidad o de masa de la muestra
  
  \section*{Principios de reducción de datos (continuación)}
  Si $\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)$ e $\mathbf{y}=\left(y_{1}, \ldots, y_{m}\right)$ son dos puntos muestrales, tales que existe una constante $c(\mathbf{x}, \mathbf{y})$ verificando que $L_{1}(\theta \mid \mathbf{x})=c(\mathbf{x}, \mathbf{y}) L_{2}(\theta \mid \mathbf{y})$, entonces la evidencia estadística que suministran ambos puntos debe ser idéntica
  
  Dos aspectos son importantes en esta definición. El primero es que la evidencia estadística se toma en un sentido amplio y no se define, así puede ser ésta, un estadístico muestral, un estadístico suficiente, un intervalo de confianza, etc. El segundo es que las dos funciones de verosimilitud no tienen por qué estar obligatoriamente definidas en el mismo espacio muestral. Realmente la evidencia estadística depende del experimento bajo estudio $E$ y del punto observado y debe expresarse como $E v(E, \mathbf{x}), E=\left(\chi^{n}, f(\mathbf{x} \mid \theta)\right)_{\theta \in \Theta \subset \mathbb{R}^{e}}$
  
  \section*{Principios de reducción de datos (continuación)}
  Ejercicio $\operatorname{Ev}\left(E_{1}, t\right)=\operatorname{Ev}\left(E_{2}, n\right)$, si\\
  $E_{1}=(\{0,1, \ldots, n\}, \operatorname{Bin}(n, \theta))_{\theta \in(0,1)}, f(t \mid \theta)=\binom{n}{t} \theta^{t}(1-\theta)^{n-t}$\\
  $E_{2}=(\mathbb{N}, B N(t, \theta))_{\theta \in(0,1)}, g(n \mid \theta)=\binom{n-1}{t-1} \theta^{t}(1-\theta)^{n-t}$\\
  Ejercicio Los procedimientos bayesianos, por estar basados en la distribución de probabilidad final o a posteriori, satisfacen el principio de verosimilitud, ya que si $\mathbf{x}=\left(x_{1}, \ldots, x_{n}\right)$ e $\mathbf{y}=\left(y_{1}, \ldots, y_{m}\right)$ satisfacen el principio de verosimilitud, entonces $\pi_{1}(\theta \mid \mathbf{x})=\pi_{2}(\theta \mid \mathbf{y})$
  
  \section*{Principio de suficiencia}
  En un experimento $E=\left(\chi^{n}, f(\mathbf{x} \mid \theta)\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$, si $T=T(\mathbf{X})$ es un estadístico suficiente para $\theta$ y se tiene que $T(\mathbf{x})=T(\mathbf{y})$, entonces $E v(E, \mathbf{x})=E v(E, \mathbf{y})$
  
  \section*{Principios de reducción de datos (continuación)}
  \section*{Principio de condicionalidad}
  Dados dos experimentos $E_{1}=\left(\chi^{n}, f_{1}(\mathbf{x} \mid \theta)\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}} y$ $E_{2}=\left(\chi^{m}, f_{2}(\mathbf{y} \mid \theta)\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}, y$ el lanzamiento de una moneda al aire representado por la v.a. $J$ tal que $P(J=1)=P(J=2)=\frac{1}{2}$, si $E=\left(\chi^{n} \cup \chi^{m} \times\{1,2\}, f(\mathbf{x}, j \mid \theta)\right)_{\theta \in \Theta \subset \mathbb{R}^{\ell}}$ es el experimento mixto representado por la v.a. $(Z, J)$ tal que $Z=\left\{\begin{array}{ll}X & \text { si } J=1 \\ Y & \text { si } J=2\end{array}, f(\mathbf{x}, 1 \mid \theta)=\frac{1}{2} f_{1}(\mathbf{x} \mid \theta), f(\mathbf{y}, 2 \mid \theta)=\frac{1}{2} f_{2}(\mathbf{y} \mid \theta)\right.$, entonces $\operatorname{Ev}(E,(\mathbf{x}, 1))=\operatorname{Ev}\left(E_{1}, \mathbf{x}\right)$ y $\operatorname{Ev}(E,(\mathbf{y}, 1))=\operatorname{Ev}\left(E_{2}, \mathbf{y}\right)$
  
  El principio de condicionalidad dice algo bastante intuitivo: mecanismos aleatorios que no dependan del valor a determinar $\theta$, no proporcionan evidencia sobre él (aleatorización en los contrastes de hipótesis para conseguir un test de tamaño determinado)
  
  \section*{Teorema de Birnbaum}
  \section*{El principio de verosimilitud es equivalente a los principios de suficiencia y condicionalidad}
  Observación El Teorema de Birnbaum es importante desde el punto de vista de los fundamentos de la Estadística. Muchos de los procedimientos estadísticos usuales violan el principio de verosimilitud, en concreto los procedimientos que se basan en la distribución en el muestreo de un estadístico pueden hacerlo. Por ejemplo, si se pasa de un modelo binomial a uno binomial negativo, la función de masa cambia y por lo tanto los IC pueden cambiar. Sin embargo, a la luz del teorema, esto significa contradecir el principio de suficiencia, que es compartido por toda aproximación a la inferencia, o el principio de condicionalidad, que parece bastante aséptico. El teorema de Birnbaum constituye uno de los motivos por los que el principio de verosimilitud no es universalmente aceptado, a pesar de que como se verá, la función de verosimilitud posee muchas buenas propiedades estadísticas
  
 